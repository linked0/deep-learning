{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "[View in Colaboratory](https://colab.research.google.com/github/linked0/deep-learning/blob/master/Batch_Normalization_Lesson_181015.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VBhtZtCVSd6r",
    "outputId": "72d072a7-b653-4183-94f7-aa4601ba8d8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.26.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Q3psD_V9S__9",
    "outputId": "f14d51bb-ce65-4acc-d435-86694cd46342"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anscombe.json\t\t      mnist_test.csv\n",
      "california_housing_test.csv   mnist_train_small.csv\n",
      "california_housing_train.csv  README.md\n"
     ]
    }
   ],
   "source": [
    "!ls sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "colab_type": "code",
    "id": "Y24eoYgHTG2D",
    "outputId": "99191b91-b19f-4035-96bb-7a7ea164a4fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-1e203c4f90fa>:9: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Import MNIST data so we have something for out experiments\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "fz4kdtRbZXPq"
   },
   "outputs": [],
   "source": [
    "class NerualNet:\n",
    "  def __init__(self, initial_weights, activation_fn, use_batch_norm):\n",
    "    # Keep track of whether or not this network uses batch normalization.\n",
    "    self.use_batch_norm = use_batch_norm\n",
    "    self.name = 'With Batch Norm' if use_batch_norm else 'Without Batch Norm'\n",
    "    \n",
    "    # Batch normalization needs to do different calculations during training and inference,\n",
    "    # so we use this placeholder to tell the graph which behavior to use.\n",
    "    self.is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "    \n",
    "    self.training_accuracies = []\n",
    "    \n",
    "    self.build_network(initial_weights, activation_fn)\n",
    "    \n",
    "  def build_network(self, initial_weights, activation_fn):\n",
    "    self.input_layer = tf.placeholder(tf.flaot32, [None, initial_weights[0].shape[0]])\n",
    "    layer_in = self.input_layer\n",
    "    for weights in initial_weights[:-1]:\n",
    "      layer_in = self.fully_connected(layer_in, weights, activation_fn)\n",
    "    self.output_layer = self.fully_connected(layer_in, initial_weights[-1])\n",
    "  \n",
    "  def fully_connected(self, layer_in, initial_weights, activation_fn=None):\n",
    "    if self.use_batch_norm and activation_fn:\n",
    "      weights = tf.Variable(initial_weights)\n",
    "      linear_output = tf.matmul(layer_in, weights)\n",
    "      \n",
    "      batch_normalized_output = tf.layers.batch_normalization(linear_output, training=self.is_training)\n",
    "      \n",
    "      return activatiion_fn(batch_normalized_output)\n",
    "    \n",
    "    else:\n",
    "      weights = tf.Variable(initial_weights)\n",
    "      biases = tf.Variable(tf.zeros([initial_weights.shape[-1]]))\n",
    "      linear_output = tf.add(tf.matmul(layer_in, weights), biases)\n",
    "      return linear_output if not activation_fn else activation_fn(linear_output)\n",
    "    \n",
    "  def train(self, session, learning_rate, training_batches, batches_per_sample, save_model_as=None):\n",
    "    \"\"\"\n",
    "    Trains the model on the MNIST training dataset\n",
    "    \n",
    "    :param session: SEssion\n",
    "      Used to run training graph operation\n",
    "    :param learning_rate: float\n",
    "      Learning rate used during gradient descent.\n",
    "    :param training_batches: int\n",
    "      Number of batches to train.\n",
    "    :param batches_per_sample: int\n",
    "      How many batches to train before sampling the validation accuracy.\n",
    "    :param save_model_as: string or None (default None)\n",
    "      Name to use if you want to save the trained model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # This placeholder will store the target labels for each mini batch\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    # Define loass and optimizaer\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=self.output_layer))\n",
    "    \n",
    "    # Define operation for testing\n",
    "    correct_prediction = tf.equal(tf.argmax(self.output_layer, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cost(correct_prediction, tf.float32))\n",
    "    \n",
    "    if self.use_batch_norm:\n",
    "      with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "    else:\n",
    "      train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "    \n",
    "    # Train for the appropriate number of batches. (tqdm is only for a nice timing display)\n",
    "    for i in tqdm.tqdm(range(training_batches)):\n",
    "      # We use batches of 60 just because the original paper did. You can use any size batch you like\n",
    "      batch_xs, batch_ys = mnint.train.next_batch(60)\n",
    "      session.run(train_step, feed_dict={self.input_layer: batch_xs,\n",
    "                                        labels: batch_ys,\n",
    "                                        self.is_training: True})\n",
    "      \n",
    "      # Periodically test accuracy against the 5k validation images and store it for plotting later.\n",
    "      if i % batches_per_sample == 0:\n",
    "        test_accuracy = session.run(accuracy, feed_dict={self.input_layer: mnist.validation.images,\n",
    "                                                        labels: mnist.validation.labels,\n",
    "                                                        self.is_traing: False})\n",
    "        self.training_accuracies.append(test_accuracy)\n",
    "        \n",
    "    # After training, report accuracy against test data\n",
    "    test_accuracy = session.run(accuracy, feed_dict={self.input_layer: mnist.validation.images,\n",
    "                                                    labels: mnist.validation.labels,\n",
    "                                                    self.is_training: False})\n",
    "    print('{}: After training, fanal accuracy on validaion set = {}'.format(self.name, test_accuracy))\n",
    "    \n",
    "    # If you want to use this model later for inference instead of having to retrain it,\n",
    "    # just construct it with the same parameters and then pass this file to the 'test' function\n",
    "    if save_model_as:\n",
    "      tf.train.Saver().save(session, save_model_as)\n",
    "    \n",
    "  def test(self, session, test_training_accuracy=False, include_individual_predictions=False, restore_from=None):\n",
    "    \"\"\"\n",
    "    Trains a trained model on the MNIST testing dataset.\n",
    "    \n",
    "    :param session: Session\n",
    "      Used to run the testing graph operations.\n",
    "    :param test_training_accurcay: bool (default False)\n",
    "      If True, perform inference with batch normalization using batch mean and varicance;\n",
    "      if False, perform inference with batch nomalization using estimated population mean and variance.\n",
    "      Note: in real life, *always* perform inference using the population mean and variance\n",
    "        This parameter exists just to support demostrating what happens if you don't\n",
    "    :param include_individual_predictions: bool (default True)\n",
    "    :param restore_from: string or None (default None)\n",
    "      Name of a saved model if you want to test with previously saved weights.\n",
    "    \"\"\"\n",
    "    # This placeholder will store the true labels for each mini batch\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    # Define operations for testing\n",
    "    correct_prediction = tf.equal(tf.argmax(self.output_layer, 1), tf.argmax(lables, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # If proveded, restore from a previously saved model\n",
    "    if restore_from:\n",
    "      tf.train.Saver().restore(session, restore_from)\n",
    "      \n",
    "    # Test against all of the MNIST test data\n",
    "    test_accuracy = session.run(accuracy, feed_dict={self.input_layer: mnist.test.images,\n",
    "                                                    labels: mnist.test.labels,\n",
    "                                                    self.is_training: test_training_accuracy})\n",
    "    print('-'*75)\n",
    "    print('{}: Accuracy on full test set = {}'.format(self.name, test_accuracy))\n",
    "    \n",
    "    # If requested, perform tests predicting individual values rather than batches\n",
    "    if include_individual_predictions:\n",
    "      predictions = []\n",
    "      correct = 0\n",
    "      \n",
    "      # Do 200 predictions, 1 at a time\n",
    "      for i in range(200):\n",
    "        # This is a normal prediction using an individual test case. However, notice\n",
    "        # we pass 'test_training_accuracy' to 'feed_dict' as the value for 'self.is_training'.\n",
    "        # Remember that will tell it whether it should use the batch mean & variance or\n",
    "        # the populatio estimates that were calculated while training the model\n",
    "        pred, corr = session.run([tf.arg_max(self.output_layer, 1), accuracy],\n",
    "                                feed_dict={self.input_layer: [mnist.test.images[i]],\n",
    "                                          labels:[mnint.test.labels[i]],\n",
    "                                          self.is_traing: test_training_accuracy})\n",
    "        correst += corr\n",
    "        predictions.append(pred[0])\n",
    "      print('200 Predcitions:', predictions)\n",
    "      print('Accuracy on 200 samples:', correct/200)\n",
    "  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Batch_Normalization_Lesson_181015.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
