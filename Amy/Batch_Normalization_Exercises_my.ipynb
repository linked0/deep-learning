{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Batch Normalization - Solutions\n",
    "Batch normalization is most useful when building deep neural networks. To demostrate this, we'll create a convolutional neural network with 20 convolutional layers, followed by fully connected layer. We'll use it to classify handwritten digits in the MNIST dataet, which should be familiar to you by now.\n",
    "\n",
    "Thhis is **not** a good network for classifying MNIST digits. You could create a muchh simpler network and get better results. However, to give you hands-on experience with batch normalization, we had to make an example that was:\n",
    "1. Complicated enough that training would benefit from batch normalization.\n",
    "2. Simple enough that it would train quickly, since this is meant to be a short exercise just to give you some practice adding batch normalization.\n",
    "3. Simple enough that the architecuture would be easy to understand without additional resources.\n",
    "\n",
    "This notebook includes two versions of the network that you can edit. This first uses higher level funtions from the tf.layers package. The second is the same network, but uses only lower level functionsin the tf.nn package.\n",
    "1. Batch Normalization with tf.layers.batch_normalization\n",
    "2. Batch Normalization with tf.nn.batch_normalization\n",
    "\n",
    "This following cell loads Tensorflow, downloads the MNIST dataset if necessary, and loads it into an object named mnist. You'll need to run this cell before running anything else in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True, reshape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS CELL\n",
    "\"\"\"\n",
    "def fully_connected(prev_layer, num_units):\n",
    "    \"\"\"\n",
    "    Create a fully connected layer with the give layer as input and the give number of neurons.\n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param num_units: int\n",
    "        The size of the layer. That is, the number of units, nodes, or neurons.\n",
    "    :returns Tensor\n",
    "        A new fully connected layer\n",
    "    \"\"\"\n",
    "    layer = tf.layers.dense(prev_layer, num_units, activation=tf.nn.relu)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the following funtion to create convolutional layers in our network. They are very basic: we're always using a 3x3 kernel, RELU activatin functions, strides of 1x1 on layers with odd depths, and strides of 2x2 on layers with even depths. We aren't bothering with pooling layers at all in this network.\n",
    "\n",
    "This version of the funtions does not include batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS CELL\n",
    "\"\"\"\n",
    "def conv_layer(prev_layer, layer_depth):\n",
    "    \"\"\"\n",
    "    Create a convolutional layer with the give layer as input\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param layer_depth: int\n",
    "        We'll set the strides and number of feature maps based on the layer's depth in the network.\n",
    "        This is *not* a good way to make a CNN, but it helps us create this example with very little code.\n",
    "    :return Tensor\n",
    "        A new convolutional layer\n",
    "    \"\"\"\n",
    "    strides = 2 if layer_depth % 3 == 0 else 1\n",
    "    conv_layer = tf.layers.conv2d(prev_layer, layer_depth*4, 3, strides, 'same', activation=tf.nn.relu)\n",
    "    return conv_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the following cell**, along with the earlier cells(to load the dataset and define the necessary functions).\n",
    "This cell builds the network **without** batch normalization, then trains it on the MNIST dataset. It displays loss and accuracy data periodically while training."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 7,
>>>>>>> f54545ae3e9dae6eb5eaf3f73d9b6c0d23e22e7e
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dense_2/BiasAdd:0\", shape=(?, 10), dtype=float32)\n",
<<<<<<< HEAD
      "Batch:  0: Validation loss: 0.69057, Validation accuracy: 0.09760\n",
      "Batch: 25: Training loss: 0.33754, Training accuracy: 0.09375\n",
      "Batch: 50: Training loss: 0.32719, Training accuracy: 0.09375\n",
      "Batch: 75: Training loss: 0.32649, Training accuracy: 0.06250\n",
      "Batch: 100: Validation loss: 0.32525, Validation accuracy: 0.11260\n",
      "Batch: 125: Training loss: 0.32623, Training accuracy: 0.03125\n",
      "Batch: 150: Training loss: 0.32583, Training accuracy: 0.10938\n",
      "Batch: 175: Training loss: 0.32365, Training accuracy: 0.09375\n",
      "Batch: 200: Validation loss: 0.32587, Validation accuracy: 0.09860\n",
      "Batch: 225: Training loss: 0.32478, Training accuracy: 0.15625\n",
      "Batch: 250: Training loss: 0.32615, Training accuracy: 0.07812\n",
      "Batch: 275: Training loss: 0.32316, Training accuracy: 0.12500\n",
      "Batch: 300: Validation loss: 0.32577, Validation accuracy: 0.09240\n",
      "Batch: 325: Training loss: 0.32312, Training accuracy: 0.15625\n",
      "Batch: 350: Training loss: 0.32346, Training accuracy: 0.12500\n",
      "Batch: 375: Training loss: 0.32656, Training accuracy: 0.09375\n",
      "Batch: 400: Validation loss: 0.32494, Validation accuracy: 0.11260\n",
      "Batch: 425: Training loss: 0.32525, Training accuracy: 0.09375\n",
      "Batch: 450: Training loss: 0.32643, Training accuracy: 0.10938\n",
      "Batch: 475: Training loss: 0.32539, Training accuracy: 0.04688\n",
      "Batch: 500: Validation loss: 0.32527, Validation accuracy: 0.10020\n",
      "Batch: 525: Training loss: 0.32777, Training accuracy: 0.12500\n",
      "Batch: 550: Training loss: 0.32341, Training accuracy: 0.20312\n",
      "Batch: 575: Training loss: 0.32215, Training accuracy: 0.20312\n",
      "Batch: 600: Validation loss: 0.32542, Validation accuracy: 0.09240\n",
      "Batch: 625: Training loss: 0.32474, Training accuracy: 0.14062\n",
      "Batch: 650: Training loss: 0.32952, Training accuracy: 0.07812\n",
      "Batch: 675: Training loss: 0.32520, Training accuracy: 0.12500\n",
      "Batch: 700: Validation loss: 0.32514, Validation accuracy: 0.09860\n",
      "Batch: 725: Training loss: 0.32527, Training accuracy: 0.09375\n",
      "Batch: 750: Training loss: 0.32535, Training accuracy: 0.14062\n",
      "Batch: 775: Training loss: 0.32568, Training accuracy: 0.12500\n",
      "Final validation accuracy: 0.11000\n",
      "Final test accuracy: 0.10280\n",
      "Accuracy on 100 samples: 0.15\n"
=======
      "Batch:  0: Validation loss: 0.69004, Validation accuracy: 0.11260\n",
      "Batch: 25: Training loss: 0.37000, Training accuracy: 0.09375\n",
      "Batch: 50: Training loss: 0.32630, Training accuracy: 0.15625\n",
      "Batch: 75: Training loss: 0.32496, Training accuracy: 0.10938\n",
      "Batch: 100: Validation loss: 0.32535, Validation accuracy: 0.09900\n",
      "Batch: 125: Training loss: 0.32635, Training accuracy: 0.06250\n",
      "Batch: 150: Training loss: 0.32320, Training accuracy: 0.15625\n",
      "Batch: 175: Training loss: 0.32810, Training accuracy: 0.14062\n",
      "Batch: 200: Validation loss: 0.32530, Validation accuracy: 0.11260\n",
      "Batch: 225: Training loss: 0.32757, Training accuracy: 0.09375\n",
      "Batch: 250: Training loss: 0.32598, Training accuracy: 0.10938\n",
      "Batch: 275: Training loss: 0.32371, Training accuracy: 0.12500\n",
      "Batch: 300: Validation loss: 0.32534, Validation accuracy: 0.09060\n",
      "Batch: 325: Training loss: 0.32623, Training accuracy: 0.10938\n",
      "Batch: 350: Training loss: 0.32395, Training accuracy: 0.12500\n",
      "Batch: 375: Training loss: 0.32631, Training accuracy: 0.12500\n",
      "Batch: 400: Validation loss: 0.32557, Validation accuracy: 0.11000\n",
      "Batch: 425: Training loss: 0.32754, Training accuracy: 0.06250\n",
      "Batch: 450: Training loss: 0.32950, Training accuracy: 0.04688\n",
      "Batch: 475: Training loss: 0.32448, Training accuracy: 0.14062\n",
      "Batch: 500: Validation loss: 0.32537, Validation accuracy: 0.09580\n",
      "Batch: 525: Training loss: 0.32521, Training accuracy: 0.12500\n",
      "Batch: 550: Training loss: 0.32632, Training accuracy: 0.12500\n",
      "Batch: 575: Training loss: 0.32794, Training accuracy: 0.06250\n",
      "Batch: 600: Validation loss: 0.32508, Validation accuracy: 0.09760\n",
      "Batch: 625: Training loss: 0.32608, Training accuracy: 0.07812\n",
      "Batch: 650: Training loss: 0.32625, Training accuracy: 0.07812\n",
      "Batch: 675: Training loss: 0.32172, Training accuracy: 0.26562\n",
      "Batch: 700: Validation loss: 0.32573, Validation accuracy: 0.09760\n",
      "Batch: 725: Training loss: 0.32331, Training accuracy: 0.10938\n",
      "Batch: 750: Training loss: 0.32558, Training accuracy: 0.04688\n",
      "Batch: 775: Training loss: 0.32642, Training accuracy: 0.07812\n",
      "Final validation accuracy: 0.09860\n",
      "Final test accuracy: 0.10100\n",
      "Accuracy on 100 samples: 0.11\n"
>>>>>>> f54545ae3e9dae6eb5eaf3f73d9b6c0d23e22e7e
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS CELL\n",
    "\"\"\"\n",
    "def train(num_batches, batch_size, learning_rate):\n",
    "    # Build placeholders for the input samples and labels\n",
    "    inputs = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    # Feed the inputs into a series of 20 convolutional layers\n",
    "    layer = inputs\n",
    "    for layer_i in range(1, 20):\n",
    "        layer = conv_layer(layer, layer_i)\n",
    "    \n",
    "    # Flatten the output from the convolutional layers\n",
    "    orig_shape = layer.get_shape().as_list()\n",
    "    layer = tf.reshape(layer, shape=[-1, orig_shape[1] * orig_shape[2] * orig_shape[3]])\n",
    "    \n",
    "    # Add one fully connected layer\n",
    "    layer = fully_connected(layer, 100)\n",
    "    \n",
    "    # Create the output layer with 1 node for each\n",
    "    logits = tf.layers.dense(layer, 10)\n",
    "    \n",
    "    print(logits)\n",
    "    \n",
    "    # Define\n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    \n",
    "    train_opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # Train and test the network\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for batch_i in range(num_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "            # train this batch\n",
    "            sess.run(train_opt, {inputs: batch_xs,\n",
    "                                labels: batch_ys})\n",
    "            \n",
    "            # Periodically check the validation or training loss and accuray\n",
    "            if batch_i % 100 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: mnist.validation.images,\n",
    "                                                             labels: mnist.validation.labels})\n",
    "                print ('Batch: {:>2}: Validation loss: {:>3.5f}, Validation accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "            elif batch_i % 25 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys})\n",
    "                print('Batch: {:>2}: Training loss: {:>3.5f}, Training accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "        \n",
    "        # At the end, score the final accuracy for bath the validation and test sets\n",
    "        acc = sess.run(accuracy, {inputs: mnist.validation.images,\n",
    "                                 labels: mnist.validation.labels})\n",
    "        print('Final validation accuracy: {:>3.5f}'.format(acc))\n",
    "        acc = sess.run(accuracy, {inputs: mnist.test.images,\n",
    "                                 labels: mnist.test.labels})\n",
    "        print('Final test accuracy: {:>3.5f}'.format(acc))\n",
    "        \n",
    "        # Score the first 100 test images individually, just to make sure batchh normalization really worked\n",
    "        correct = 0\n",
    "        for i in range(100):\n",
    "            correct += sess.run(accuracy, feed_dict={inputs: [mnist.test.images[i]],\n",
    "                                                    labels: [mnist.test.labels[i]]})\n",
    "        print('Accuracy on 100 samples:', correct/100)\n",
    "        \n",
    "num_batches = 800\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    train(num_batches, batch_size, learning_rate)"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Add batch normalization\n",
    "To add batchh normalization to the layers created by fully_conntected, we did the following:\n",
    "1. Added the `is_training` parameter to the function signature so we can pass that information to the batch normalization layer.\n",
    "2. Removed the bias and activation function from the dense layer.\n",
    "3. Used `tf.layers.batch_normalization` to normalize the layer's output. Notice we pass `is_training` to this layer to ensure the network updates its population statistics appropriately.\n",
    "4. Passed the normalized values into a ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_connected(prev_layer, num_units, is_training):\n",
    "    \"\"\"\n",
    "    Create a fully connected layer with thhe given layer as input and the given number of neurons.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param num_units: int\n",
    "        The size of the layer. That is, the number of units, nodes, or neurons.\n",
    "    :param is_training: bool or Tensor\n",
    "        Indicates whether or not the notwork is currently training, which tells the batch normalization\n",
    "        layer whether or not it should update or use its population statistics.\n",
    "    :returns Tensor\n",
    "        A new fully connected layer\n",
    "    \"\"\"\n",
    "    layer = tf.layers.dense(prev_layer, num_units, use_bias=False, activation=None)\n",
    "    layer = tf.layers.batch_normalization(layer, training=is_training)\n",
    "    layer = tf.nn.relu(layer)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compare this function to `fully_connected`, you'll see that - when using tf.layers - there really isn't any difference between normalizing a fully connected layer and a convolutional layer. However, if you look at the second example in this notebook, where we restrict ourselves to the tf.nn package, you'll see a small difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(prev_layer, layer_depth, is_training):\n",
    "    strides = 2 if layer_depth % 3 == 0 else 1\n",
    "    conv_layer = tf.layers.conv2d(prev_layer, layer_depth*4, 3, strides, 'same', use_bias=False, activation=None)\n",
    "    conv_layer = tf.layers.batch_normalization(conv_layer, training=is_training)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    \n",
    "    return conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0: Validation loss: 0.69123, Validation accuracy: 0.09760\n",
      "Batch: 25: Training loss: 0.56436, Training accuracy: 0.10938\n",
      "Batch: 50: Training loss: 0.43716, Training accuracy: 0.15625\n",
      "Batch: 75: Training loss: 0.37141, Training accuracy: 0.04688\n",
      "Batch: 100: Validation loss: 0.34000, Validation accuracy: 0.09240\n",
      "Batch: 125: Training loss: 0.33479, Training accuracy: 0.06250\n",
      "Batch: 150: Training loss: 0.33638, Training accuracy: 0.10938\n",
      "Batch: 175: Training loss: 0.37132, Training accuracy: 0.04688\n",
      "Batch: 200: Validation loss: 0.37157, Validation accuracy: 0.11280\n",
      "Batch: 225: Training loss: 0.44167, Training accuracy: 0.21875\n",
      "Batch: 250: Training loss: 0.35018, Training accuracy: 0.40625\n",
      "Batch: 275: Training loss: 0.21052, Training accuracy: 0.59375\n",
      "Batch: 300: Validation loss: 0.10167, Validation accuracy: 0.83780\n",
      "Batch: 325: Training loss: 0.03958, Training accuracy: 0.95312\n",
      "Batch: 350: Training loss: 0.04192, Training accuracy: 0.93750\n",
      "Batch: 375: Training loss: 0.02245, Training accuracy: 0.96875\n",
      "Batch: 400: Validation loss: 0.03288, Validation accuracy: 0.95220\n",
      "Batch: 425: Training loss: 0.04335, Training accuracy: 0.92188\n",
      "Batch: 450: Training loss: 0.02417, Training accuracy: 0.95312\n",
      "Batch: 475: Training loss: 0.05893, Training accuracy: 0.87500\n",
      "Batch: 500: Validation loss: 0.03203, Validation accuracy: 0.95200\n",
      "Batch: 525: Training loss: 0.05211, Training accuracy: 0.89062\n",
      "Batch: 550: Training loss: 0.08513, Training accuracy: 0.92188\n",
      "Batch: 575: Training loss: 0.02592, Training accuracy: 0.95312\n",
      "Batch: 600: Validation loss: 0.04529, Validation accuracy: 0.94040\n",
      "Batch: 625: Training loss: 0.03750, Training accuracy: 0.95312\n",
      "Batch: 650: Training loss: 0.02362, Training accuracy: 0.98438\n",
      "Batch: 675: Training loss: 0.00126, Training accuracy: 1.00000\n",
      "Batch: 700: Validation loss: 0.04293, Validation accuracy: 0.93900\n",
      "Batch: 725: Training loss: 0.03416, Training accuracy: 0.95312\n",
      "Batch: 750: Training loss: 0.02070, Training accuracy: 0.96875\n",
      "Batch: 775: Training loss: 0.00899, Training accuracy: 0.96875\n",
      "Final validation accuracy: 0.94860\n",
      "Final test accuracy: 0.94490\n",
      "Accuracy on 100 samples: 0.99\n"
     ]
    }
   ],
   "source": [
    "def train(num_batches, batch_size, learning_rate):\n",
    "    # Build placeholders for the input samples and labels\n",
    "    inputs = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    # Add placeholder to indicate whether or not we're training the model\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    # Feed thhe inputs into a sereis of 20 convolutional layers\n",
    "    layer = inputs\n",
    "    for layer_i in range(1, 20):\n",
    "        layer = conv_layer(layer, layer_i, is_training)\n",
    "        \n",
    "    # Flatten the output from thhe convolutional layers\n",
    "    orig_shape = layer.get_shape().as_list()\n",
    "    layer = tf.reshape(layer, shape=[-1, orig_shape[1] * orig_shape[2] * orig_shape[3]])\n",
    "    \n",
    "    # Add one fully connected layer\n",
    "    layer = fully_connected(layer, 100, is_training)\n",
    "    \n",
    "    # Create the ouput layer withh 1 node for each\n",
    "    logits = tf.layers.dense(layer, 10)\n",
    "    \n",
    "    # Define loss and training operations\n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    \n",
    "    # Tell TensorFlow to update the population statistics while training\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        train_opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "    \n",
    "    # Create operation to test accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # Train and test the network\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for batch_i in range(num_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "            # train this batch\n",
    "            sess.run(train_opt, {inputs: batch_xs, labels: batch_ys, is_training: True})\n",
    "            \n",
    "            # Periodically check the validation or training loss and accuracy\n",
    "            if batch_i % 100 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: mnist.validation.images,\n",
    "                                                             labels: mnist.validation.labels,\n",
    "                                                             is_training: False})\n",
    "                print('Batch: {:>2}: Validation loss: {:>3.5f}, Validation accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "            elif batch_i % 25 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys, is_training: False})\n",
    "                print('Batch: {:>2}: Training loss: {:>3.5f}, Training accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "        \n",
    "        # At the end, score the final accuracy for both the validation and test sets\n",
    "        acc = sess.run(accuracy, {inputs: mnist.validation.images,\n",
    "                                 labels: mnist.validation.labels,\n",
    "                                 is_training: False})\n",
    "        print('Final validation accuracy: {:>3.5f}'.format(acc))\n",
    "        acc = sess.run(accuracy, {inputs: mnist.test.images,\n",
    "                                 labels: mnist.test.labels,\n",
    "                                 is_training: False})\n",
    "        print('Final test accuracy: {:>3.5f}'.format(acc))\n",
    "        \n",
    "        # Score the first 100 test images individually, just to make sure batch normalization really worked\n",
    "        correct = 0\n",
    "        for i in range(100):\n",
    "            correct += sess.run(accuracy, feed_dict={inputs: [mnist.test.images[i]],\n",
    "                                                    labels: [mnist.test.labels[i]],\n",
    "                                                    is_training: False})\n",
    "        print('Accuracy on 100 samples:', correct/100)\n",
    "        \n",
    "\n",
    "num_batches = 800\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    train(num_batches, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With batch normalization, we now get excellent performace. In fact, validation accuracy is almost 94% after only 500 batches. Notice alose thhe last line of the output: `Accuracy on 100 samples`. If thhis value is low while everything else looks good, that means you did not implement batchh normalization correctly. Sepdifically, it means you either did not calculate the population mean and variance while training, or you are not using those values during inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization using tf.nn.batch_normalization"
   ]
  },
  {
=======
>>>>>>> f54545ae3e9dae6eb5eaf3f73d9b6c0d23e22e7e
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
